---
title: "Practical Machine Learning Final Project"
author: "Emily Payne"
output: html_document
---

## Summary
My goal is to be able to predict how well a participant performed a movement based on accelerometer data gathered from the [Weight Lifting Exercises Dataset] (http://groupware.les.inf.puc-rio.br/har). This data tracks the accelerometers on the belt, forearm, arm, and dumbell of 6 participants. Each participant was asked to perform barbell lifts correctly and incorrectly 5 different ways, as recorded in the "classe" variable. This variable holds the value "A" if the movement was performed exactly to specification, "B" if elbows were thrown in front, etc. In this project, I will work to find the best method to predict the "classe" variable- or how an activity was performed- given the rest of the accelerometer data. 

## Loading and Preprocessing the Data
To ensure that these results are reproducible, we set the seed:
``` {r}
set.seed(123456)
```
And now we load the data necessary libraries:
``` {r echo = T, results = 'hide'}
library(caret)
library(kernlab)
library(rattle)
library(ggplot2)
download.file(url='https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv', destfile='pml-training.csv', method='curl')
download.file(url='https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv', destfile='pml-testing.csv', method='curl')
pmlTrainData <- read.csv(file = "pml-training.csv", header = TRUE, sep = ",")
pmlTestData <- read.csv(file = "pml-testing.csv", header = TRUE, sep = ",")
```
Now we can filter out the first few variables that we know won't be great predictors (for example: name, timestamp, etc.)
```{r}
pmlTrainData <- pmlTrainData[,-(1:7)]
pmlTestData <- pmlTestData[,-(1:7)]
```
Also, there are some bad values in this set, "#DIV/0!" that should just be labeled NA. Then we filter out the variables that are over 50% NA values, because if we kept these and imputed the missing values, it would introduce significant inaccuracies / bias to the analysis.
```{r}
newNATrain <- sapply(pmlTrainData, function(y) y == "#DIV/0!")
pmlTrainData[newNATrain] <- NA
naPercent <-sapply(pmlTrainData, function(y) sum(length(which(is.na(y))))/length(pmlTrainData$classe))
naPercent <- data.frame(naPercent)
naPercent$name <- rownames(naPercent)
colsExclude <- naPercent$name[naPercent$naPercent > 0.5]
pmlTrainData <- pmlTrainData[, -which(names(pmlTrainData) %in% colsExclude)]
pmlTestData <- pmlTestData[, -which(names(pmlTestData) %in% colsExclude)]
```
Remove variables that are near zero variance, then split the trianing data into a testing and training dataset so that we can perform cross validation and calculate the out of sample error. We are going to use the k-fold cross validation method, with k = 5.
```{r}
nsv <- nearZeroVar(pmlTrainData)
pmlTrainData <- pmlTrainData[, -nsv]
pmlTestData <- pmlTestData[, -nsv]
inTrain <- createDataPartition(y = pmlTrainData$classe, p = 0.7, list = FALSE)
training <- pmlTrainData[inTrain,]
testing <- pmlTrainData[-inTrain,]
trainControl <- trainControl(method = "cv", number = 5)
```
We are going to use this data for three different classification models, then determine which one is the most accurate.

## Method 1: Classification Tree
First, we will try trianing a model using the classification tree algorithm:
```{r}
modelTree <- train(classe ~., data = training, method = "rpart", trControl = trainControl)
```
We can plot this classification tree:
```{r}
fancyRpartPlot(modelTree$finalModel)
```

We can then test the accuracy of this model by taking the confusion matrix of the values that the model predicts:
```{r}
predictTree <- predict(modelTree, testing)
CT <- confusionMatrix(predictTree, testing$classe)
CT$overall[1]
```
The accuracy is 0.4973662, and so the predicted out of sample error rate is 0.5026338. So this model is as accurate as just flipping a coin. To visualize this, we can plot the normalized confusion matrix, just like on the Weight Lifting Exercises dataset website:
```{r}
normalize <- function(x) { 
     x <- sweep(x, 2, apply(x, 2, min)) 
     sweep(x, 2, apply(x, 2, max), "/") 
  }
normalizedT <- normalize(CT$table)
confusionT <- as.data.frame(as.table(normalizedT))
plot <- ggplot(confusionT)
plot + geom_tile(aes(x=Reference, y=Prediction, fill=Freq)) + scale_x_discrete(name="Actual Class") + scale_y_discrete(name="Predicted Class")+ scale_fill_gradient(breaks=seq(from=0, to=1, by=.1)) + labs(fill="Normalized\nFrequency") + geom_text(aes(x = Reference, y = Prediction, label = round(Freq/sum(Freq), 6)))
```

## Method 2: Random Forest
Now we will try a model that is known for its accuracy: random forest.
```{r}
modelRF <- train(classe ~., data = training, method = "rf", trControl = trainControl, prox = TRUE)
predictRF <- predict(modelRF, testing)
CRF <- confusionMatrix(predictRF, testing$classe)
CRF$overall[1]
```
The accuracy is 0.9884452, and so the predicted out of sample error rate is 0.0115548. This is much more accurate, however, it takes awhile to run.
```{r}
normalizedRF <- normalize(CRF$table)
confusionRF <- as.data.frame(as.table(normalizedRF))
plot <- ggplot(confusionRF)
plot + geom_tile(aes(x=Reference, y=Prediction, fill=Freq)) + scale_x_discrete(name="Actual Class") + scale_y_discrete(name="Predicted Class")+ scale_fill_gradient(breaks=seq(from=0, to=1, by=.1)) + labs(fill="Normalized\nFrequency") + geom_text(aes(x = Reference, y = Prediction, label = round(Freq/sum(Freq), 6)))
```

## Method 3: Boosting with Trees
The last model we'll try is boosting with trees.
```{r}
modelGBM <- train(classe ~., data = training, method = "gbm", trControl = trainControl, verbose = FALSE)
predictGBM <- predict(modelGBM, testing)
CGBM <- confusionMatrix(predictGBM, testing$classe)
CGBM$overall[1]
```
The accuracy is 0.9592184, and so the predicted out of sample error rate is 0.0407816. So this method is not as accurate as a random forest model, but it takes much less time to execute.
```{r}
normalizedGBM <- normalize(CGBM$table)
confusionGBM <- as.data.frame(as.table(normalizedGBM))
plot <- ggplot(confusionGBM)
plot + geom_tile(aes(x=Reference, y=Prediction, fill=Freq)) + scale_x_discrete(name="Actual Class") + scale_y_discrete(name="Predicted Class")+ scale_fill_gradient(breaks=seq(from=0, to=1, by=.1)) + labs(fill="Normalized\nFrequency") + geom_text(aes(x = Reference, y = Prediction, label = round(Freq/sum(Freq), 6)))
```


## Conclusion
The random forest method yeilded the most accurate results, so we will apply this algorithm to the 20 test cases:
```{r}
predictRFTest <- predict(modelRF, pmlTestData)
predictRFTest
```
These are the predicted values based on our random forest model.




